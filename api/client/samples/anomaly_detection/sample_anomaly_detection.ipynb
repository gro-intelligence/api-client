{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Tool\n",
    "In this notebook, we outline a process for identifying anomalies in geospatial data. For example, this notebook can be used to set up alerts for data series. This example focuses on monitoring when the amount of rainfall or NDVI is higher or lower than usual in a certain area. Below, you can find some helpful frameworks for pulling a batch of data for multiple regions, transforming that data into a more statistically significant format, identifying key properties of that data, and then identifying if recent values are anomalous according to two sigma significance. This code is extendable to include different tests for significance, different data sets and different use cases.\n",
    "\n",
    "## Set up the coding environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from groclient import GroClient\n",
    "from datetime import datetime, date, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "from groclient.lib import REGION_LEVELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GROAPI_TOKEN = os.environ['GROAPI_TOKEN']\n",
    "API_HOST = 'api.gro-intelligence.com'\n",
    "gclient = GroClient(API_HOST, GROAPI_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first example, we'll investigate weather conditions for regions within Salta, a major soybean producing region of Argentina. In this step, we'll identify the IDs for all of the counties within Salta. If we wish to discover anomalies in another region, we can simply change the region_id to that of our region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "district ids:\n",
      "[102446, 102454, 102459, 102463, 102453, 102449, 102458, 102451, 102448, 102457, 102447, 102465, 102467, 102462, 102464, 102455, 102468, 102461, 102450, 102452, 102460, 102456, 102466]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_name = \"Salta Province in Argentina Temperature Anomalies\"\n",
    "region_id = 10152 # This is the region id for salta from the gro platform\n",
    "districts_list = [region['id'] for region in gclient.get_descendant_regions(region_id, REGION_LEVELS['district'])] #districts are region level 5 in Gro's ontology\n",
    "print(\"district ids:\")\n",
    "print(districts_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Land Temperature Anomalies\n",
    "\n",
    "Since Gro can have multiple sources for a given data series, we will need a function to help us obtain the best data series for multiple locations. We want to choose the best series every time, so we'll use the rank_series_by_source method to retrieve the source with the best data coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def best_series_picker(selection):\n",
    "    '''\n",
    "    chooses the series with the greatest density and history of datapoints\n",
    "    '''\n",
    "    for series in gclient.rank_series_by_source(selection):\n",
    "        return gclient.get_data_series(**series)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a method to pull data from gro using the desired regions, metric, and item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_for_multiple_regions(regions_list, metric_id, item_id):\n",
    "    selections = []\n",
    "    for region in regions_list:\n",
    "        '''\n",
    "        ids from app.gro-intelligence.com\n",
    "        '''\n",
    "        try:\n",
    "            # You can get these id's from our web app at gro-intelligence.com\n",
    "            selections.append(best_series_picker([{'metric_id': metric_id, 'item_id': item_id, 'region_id': region}]))    \n",
    "        except:\n",
    "            print(\"no data for region \",region)\n",
    "    output = []\n",
    "    for set_of_ids in selections:\n",
    "        if(set_of_ids!= None):\n",
    "            output.append(gclient.get_data_points(**set_of_ids))\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to pull data from the Gro API and examine what we have. (This may take a few seconds)\n",
    "\n",
    "You can get the ids necessary to pull a data series at gro-intelligence.com by searching for the terms you need. in this case we are going to look at Land temperature (daytime, modeled) from the MOD11 Satellite housed on gro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You can access ids in the web app at gro-intelligence.com\n",
    "metric_id = 2540047 # Temperature\n",
    "item_id = 3457 # Land temperature (daytime, modeled)\n",
    "regions_list = districts_list\n",
    "land_temperature_array = get_data_for_multiple_regions(regions_list, metric_id, item_id) \n",
    "# We will use land_temperature_array later!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check on that data by printing one data point! (if you try to print the whole thing the notebook will crash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start_date': '2020-01-09T00:00:00.000Z', 'end_date': '2020-01-09T00:00:00.000Z', 'value': 38.8685747587904, 'input_unit_id': 36, 'input_unit_scale': 1, 'unit_id': 36, 'metric_id': 2540047, 'item_id': 3457, 'region_id': 102446, 'frequency_id': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "first_location_data = land_temperature_array[0]\n",
    "most_recent_data_point = first_location_data[-1]\n",
    "print(most_recent_data_point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at what we are working with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most recently reported value 38.8685747587904\n",
      "item name: Land temperature (daytime, modeled)\n",
      "unit name: Celsius\n",
      "name of region: Anta\n",
      "name of metric: Temperature\n",
      "last reported data point:  on 2020-01-09 \n",
      "frequency reported: daily\n",
      "time period (years): 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "most_recent_value = (most_recent_data_point['value'])\n",
    "item = gclient.lookup('items',first_location_data[0]['item_id'])['name']\n",
    "unit = gclient.lookup('units',first_location_data[0]['unit_id'])['name']\n",
    "region = gclient.lookup('regions',first_location_data[0]['region_id'])['name']\n",
    "metric_id = first_location_data[0]['metric_id']\n",
    "metric_name = gclient.lookup('metrics',metric_id)['name']\n",
    "time_stamp = \" on \" + most_recent_data_point['end_date'][:10] + \" \"\n",
    "frequency = gclient.lookup('frequencies',first_location_data[0]['frequency_id'])['name']\n",
    "TIMESTAMP_FORMAT = '%Y-%m-%dT%H:%M:%S.%fZ'\n",
    "years = datetime.strptime(most_recent_data_point['end_date'], TIMESTAMP_FORMAT).year - datetime.strptime(first_location_data[0]['end_date'], TIMESTAMP_FORMAT).year\n",
    "\n",
    "print(\"most recently reported value\", most_recent_value)\n",
    "print(\"item name:\", item)\n",
    "print(\"unit name:\", unit)\n",
    "print(\"name of region:\", region)\n",
    "print(\"name of metric:\", metric_name)\n",
    "print(\"last reported data point:\", time_stamp)\n",
    "print(\"frequency reported:\", frequency)\n",
    "print(\"time period (years):\", years)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will want to give our users friendly wording when they read their anomaly alerts\n",
    "this is optional, but for many readers \"vegetation health\" will help a user interpret \"NDVI.\"\n",
    "The variable: 'data' is defined as one data series from the land_temperature_array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAINFALL_METRIC_ID = 2100031\n",
    "SOIL_MOISTURE_METRIC_ID = 15531082\n",
    "NDVI_METRIC_ID = 431132\n",
    "ETA_METRIC_ID = 2010042\n",
    "TEMP_METRIC_ID = 2540047\n",
    "\n",
    "def check_if_geospatial(data):\n",
    "    metric_id = data[0]['metric_id']\n",
    "    if(metric_id==RAINFALL_METRIC_ID):# Trmm\n",
    "        metric_name = '7 day cumulative rainfall'\n",
    "    elif(metric_id==SOIL_MOISTURE_METRIC_ID):# Soil Moisture\n",
    "        metric_name = 'soil moisture'\n",
    "    elif(metric_id==NDVI_METRIC_ID):# NDVI\n",
    "        metric_name = 'vegetation health (NDVI)'\n",
    "    elif(metric_id==ETA_METRIC_ID):# ETA\n",
    "        metric_name = 'evapotranspiration'\n",
    "    elif(metric_id==TEMP_METRIC_ID):# Temp\n",
    "        metric_name = '7 day average temperature'\n",
    "    else:\n",
    "        return False, ''\n",
    "    return True, metric_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geospatial data: True\n",
      "type: 7 day average temperature\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Lets try it out on our first data location\n",
    "is_geospatial, new_metric_name = check_if_geospatial(first_location_data)\n",
    "print(\"geospatial data:\", is_geospatial)\n",
    "print(\"type:\", new_metric_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data for more significant signals\n",
    "\n",
    "For some data series we dont want to look at values as frequently as they are reported, instead we want to look at 7-day cumulative or 7-day average values. Here we will modify those specified series and Temperature is one of them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated frequency: 7-day average\n",
      "most recent transformed value: 31.458684808882985\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def transform_by_data_type(df, current_value, frequency):\n",
    "    '''\n",
    "    change frequency for certain weather items\n",
    "    '''\n",
    "    metric_id = df.loc[0,'metric_id']\n",
    "    if(metric_id == 2540047 or metric_id == 15531082): # 2540047 is Land temp, 15531082 is soil moisture (MEAN)\n",
    "        df = df.fillna(df.mean())\n",
    "        df['value'] = df['value'].rolling(7).mean()\n",
    "        frequency = \"7-day average\"\n",
    "    elif( metric_id == 2100031):\n",
    "        df = df.fillna(0)\n",
    "        df['value'] = df['value'].rolling(7).sum() # 2100031 is rainfall (SUM)\n",
    "        frequency = \"7-day cumulative\"\n",
    "    else:\n",
    "        return df, current_value, frequency\n",
    "    df = df.iloc[::7, :]\n",
    "    df.index = pd.RangeIndex(len(df.index))\n",
    "    current_value = df.iloc[-1]['value']\n",
    "    return df, current_value, frequency\n",
    "\n",
    "\n",
    "# Get percent change from average\n",
    "df = pd.DataFrame(first_location_data)\n",
    "\n",
    "df, current_value, frequency = transform_by_data_type(df, most_recent_value, frequency)\n",
    "\n",
    "print(\"updated frequency:\", frequency)\n",
    "print(\"most recent transformed value:\", current_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a significance / anomaly test\n",
    "\n",
    "Now we need to find the lower and upper bounds for our testing for significance, we have chosen to consider two standard deviations from the mean. After that we will use the bounds to test for significant deviations from the mean and record highs and lows. This test could be replaced with any other test for significance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower bound: 16.17364207751991\n",
      "upper bound: 35.84491956150512\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def find_lower_and_upper(df,standard_devs):\n",
    "    '''\n",
    "    Standard Deviation Calculations for Alerts:\n",
    "    Using standard deviations we cand find out if recent numbers\n",
    "    reported to the user are abnormal or to be expected\n",
    "    Returns upper and lower bounds for a series and a given number of standard deviations\n",
    "    '''\n",
    "    mean = df.mean()\n",
    "    std = df.std()\n",
    "    lower_bound = (mean - (standard_devs * std))\n",
    "    upper_bound = (mean + (standard_devs * std))\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "\n",
    "df_lower_upper = df['value']\n",
    "standard_deviations = 2\n",
    "lower_bound, upper_bound = find_lower_and_upper(df_lower_upper,standard_deviations)\n",
    "\n",
    "print(\"lower bound:\", lower_bound)\n",
    "print(\"upper bound:\", upper_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test result: Not significant for period: 20 years \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def selected_tests(df,lower_bound,upper_bound,current_value):\n",
    "    is_significant = False\n",
    "    statement = \"\"\n",
    "    if(current_value == df[\"value\"].max()):\n",
    "        is_significant = True\n",
    "        statement = \"record high in \"\n",
    "    elif(current_value == df[\"value\"].min()):\n",
    "        is_significant = True\n",
    "        statement = \"record low in \"\n",
    "    elif(current_value<=lower_bound):\n",
    "        is_significant = True\n",
    "        statement = \"significant low in \"\n",
    "    elif(current_value>=upper_bound):\n",
    "        is_significant = True\n",
    "        statement = \"significant high in \"\n",
    "    else:\n",
    "        statement = \"Not significant for period: \"\n",
    "    return is_significant, statement\n",
    "\n",
    "\n",
    "is_significant, statement = selected_tests(df,lower_bound,upper_bound,current_value)\n",
    "\n",
    "statement += str(years) + \" years \"\n",
    "current_value = ((round(current_value,2)))\n",
    "\n",
    "print(\"test result:\", statement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to contextualize this number by saying exactly what the deviation from the mean looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent deviation: 21% \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_percent_change_statement(number, avg, metric_id):\n",
    "    if(metric_id == 2010042): # ETA\n",
    "        percent_val = number/100\n",
    "        deviation = number - 100\n",
    "    elif(metric_id == 431132): # NDVI\n",
    "        percent_val = abs(number)\n",
    "        deviation = number\n",
    "    else:\n",
    "        deviation = (number) - (avg)\n",
    "        if(avg!=0):\n",
    "            percent_val = (abs(deviation) / abs(avg))\n",
    "        else:\n",
    "            return ''\n",
    "    # Format string response\n",
    "    if(deviation>=0):\n",
    "        statement = str(\"{0:.0%}\".format(percent_val)) + \" above average which is a\"\n",
    "    else:\n",
    "        statement = str(\"{0:.0%}\".format(percent_val)) + \" below average which is a\"\n",
    "\n",
    "    return statement\n",
    "\n",
    "mean = df[\"value\"].mean()\n",
    "percent_change = get_percent_change_statement(current_value, mean, metric_id)\n",
    "\n",
    "print(\"percent deviation:\", percent_change[:4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "After walking through the process we want to add some methods so we can integrate this into our workflow and use it for many data series at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argentina_string:\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def current_is_significant(data):\n",
    "    '''\n",
    "    Returns bool True if the latest value is the global max\n",
    "    '''\n",
    "    current_value = (data[-1]['value'])\n",
    "    frequency = gclient.lookup('frequencies',data[0]['frequency_id'])['name']\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df, current_value, frequency = transform_by_data_type(df, current_value, frequency)\n",
    "    current_value = df.iloc[-1]['value']\n",
    "\n",
    "    df_lower_upper = df['value']\n",
    "    standard_deviations = 2\n",
    "    lower_bound, upper_bound = find_lower_and_upper(df_lower_upper,standard_deviations)\n",
    "    \n",
    "    is_significant, statement = selected_tests(df,lower_bound,upper_bound,current_value)\n",
    "    \n",
    "    # Get frequency\n",
    "    years = datetime.strptime(data[-1]['end_date'], TIMESTAMP_FORMAT).year - datetime.strptime(data[0]['end_date'], TIMESTAMP_FORMAT).year\n",
    "    statement += str(years) + \" years \"\n",
    "    return is_significant, statement, ((round(current_value,2)))\n",
    "\n",
    "\n",
    "def return_alerts(data):\n",
    "    alert = \"\"\n",
    "    '''\n",
    "    alert if current data is significant in gro series\n",
    "    make sure there was data in the original request before running the rest\n",
    "    '''\n",
    "    # Make sure the most recent reporting date is within the last 30 days\n",
    "    current_date = datetime.now()\n",
    "    try:\n",
    "        # Make sure there are more than 3 data points\n",
    "        assert(len(data) > 3)  \n",
    "        most_recent = data[-1]['reporting_date']\n",
    "        data_date = datetime.strptime(most_recent, TIMESTAMP_FORMAT)\n",
    "    except:\n",
    "        most_recent = data[-1]['end_date']\n",
    "        data_date = datetime.strptime(most_recent, TIMESTAMP_FORMAT)\n",
    "    delta = current_date-data_date\n",
    "    days = int(str(delta).split()[0])\n",
    "    if(days>=30):\n",
    "        return \"\"\n",
    "    \n",
    "    # Alert if current is max\n",
    "    bin_val1,statement1,val1 = current_is_significant(data)\n",
    "    if(bin_val1):\n",
    "        alert += \" \" + statement1\n",
    "    \n",
    "    ''' You can add tests for seasonal significance, tests for alternate distributions etc.\n",
    "    bin_val2,statement2,val2 = YOUR_TEST_HERE(data)  \n",
    "    if(bin_val2):\n",
    "        alert += \" \" + statement2\n",
    "    '''\n",
    "    return alert\n",
    "\n",
    "\n",
    "def string_formatting(data, number):\n",
    "    try:\n",
    "        # Make sure there was data returned \n",
    "        most_recent_value = (data[number]['value'])\n",
    "        \n",
    "        # Provide metadata for the reader from gro api client and format into human readable string\n",
    "        item = gclient.lookup('items',data[0]['item_id'])['name']\n",
    "        unit = gclient.lookup('units',data[0]['unit_id'])['name']\n",
    "        region = gclient.lookup('regions',data[0]['region_id'])['name']\n",
    "        metric_id = data[0]['metric_id']\n",
    "        metric_name = gclient.lookup('metrics',metric_id)['name']\n",
    "        time_stamp = \" on \" + data[number]['end_date'][:10] + \" \"\n",
    "        frequency = gclient.lookup('frequencies',data[0]['frequency_id'])['name']\n",
    "        \n",
    "    except (TypeError, KeyError):\n",
    "        print(\"type error or key error in string_formatting\")\n",
    "        return \"\", ''\n",
    "\n",
    "    # Get percent change from average\n",
    "    df = pd.DataFrame(data)\n",
    "    df, current_value, frequency = transform_by_data_type(df, most_recent_value, frequency)\n",
    "    mean = df[\"value\"].mean()\n",
    "    percent_change = get_percent_change_statement(most_recent_value, mean, metric_id)\n",
    "\n",
    "    # Check if this is a geospatial data series\n",
    "    is_geospatial, new_metric_name = check_if_geospatial(data)\n",
    "\n",
    "    # If this IS a geospatial metric\n",
    "    if(is_geospatial):\n",
    "        # Create string\n",
    "        significance_headline = region+ \" \" + new_metric_name + \": \" + percent_change\n",
    "\n",
    "    # If this is NOT a geospatial metric\n",
    "    else:\n",
    "        # Make sure it exists\n",
    "        if(most_recent_value):\n",
    "            most_recent_value = round(most_recent_value,2)\n",
    "        else:\n",
    "            most_recent_value = 0\n",
    "\n",
    "        # Adds commas and gets rid of decimals\n",
    "        if(most_recent_value>100):\n",
    "            most_recent_value = (format(int(most_recent_value), ',d'))\n",
    "        else:\n",
    "            most_recent_value = str(most_recent_value)\n",
    "        # Create string\n",
    "        item_metric = item + \" \" + metric_name\n",
    "        significance_headline = region+ \" \" + item_metric + \": \" + most_recent_value + \" \" + unit + \", \" + percent_change\n",
    "    return significance_headline\n",
    "\n",
    "\n",
    "def work_to_distribute(data):\n",
    "    '''\n",
    "    Method run by workers, this will run each of the tests in return_alerts\n",
    "    '''\n",
    "    string_add = \"\"\n",
    "    data_line_2 = return_alerts(data)\n",
    "    if(data_line_2!=\"\"):\n",
    "        data_line_1 = string_formatting(data,-1)\n",
    "        string_add += (data_line_1 + data_line_2 + \"\\n\")\n",
    "    return string_add\n",
    "\n",
    "\n",
    "def compute_amomalies(array, data_name):\n",
    "    '''\n",
    "    Takes array of data series and a category name and returns any alerts string formatted under a header\n",
    "    '''\n",
    "    string = \"\"\n",
    "    string_add = \"\"\n",
    "    string_list = []\n",
    "    if(array != None):\n",
    "        try:\n",
    "            for ele in array:\n",
    "                string_list.append(work_to_distribute(ele))\n",
    "        except IndexError:\n",
    "            print(\"IndexError in compute_amomalies for \" + data_name)\n",
    "            return string_add\n",
    "            \n",
    "        for x in string_list:\n",
    "            string_add += x\n",
    "        if(string_add!=\"\"):\n",
    "            string = data_name\n",
    "        else:\n",
    "            string = \"\"\n",
    "\n",
    "    data_statement = string + \" \\n\" + string_add\n",
    "\n",
    "    return data_statement\n",
    "\n",
    "\n",
    "argentina_string = compute_amomalies(land_temperature_array, data_name)\n",
    "print(\"argentina_string:\")\n",
    "print(argentina_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDVI Anomalies in Brazil\n",
    "\n",
    "Now that we have our alerts from Argentina about Land Temperature (If the cell above prints nothing, that means there were no alerts for significant temperature in Argentina), we will try brazil NDVI (Normalized Difference Vegetation Index) anomalies as well. Our data source for NDVI allows us to estimate the amount of chlorophyll in plants in a given region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110117, 110031, 110130, 110097, 110120, 110059, 110124, 110047, 110041, 110118, 110055, 110061, 110084, 110082, 110096, 110077, 110109, 110100, 109997, 110045, 110021, 110074, 109998, 110022, 110024, 110086, 110001, 109995, 110048, 110064, 110004, 110098, 110016, 110037, 110010, 110078, 110107, 109999, 110051, 110017, 110091, 110116, 110023, 110131, 110089, 110065, 110088, 110028, 110002, 110090, 110115, 110092, 110018, 110104, 110052, 110056, 110011, 110106, 110085, 110083, 110035, 110122, 110087, 110121, 110006, 109996, 110068, 110014, 110034, 110057, 110066, 110102, 110042, 110071, 110039, 110053, 110040, 110007, 110094, 110049, 110030, 110073, 110029, 110129, 110076, 110105, 110075, 110067, 110062, 110113, 110114, 110000, 110020, 110046, 110050, 110063, 110101, 110110, 110128, 110036, 110093, 110069, 110012, 110003, 110103, 110080, 110127, 110099, 110060, 110015, 110008, 110033, 110070, 110027, 110095, 110079, 110112, 110019, 110013, 110005, 110125, 110111, 110108, 110072, 110119, 110126, 110032, 110026, 110054, 110123, 110025, 110009, 110044, 110038, 110043, 110081, 110058, 100022399, 100022400, 100022401]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tocantins Province in Brazil\n",
    "tocantins_province_id = 10428\n",
    "districts_list_brazil = [region['id'] for region in gclient.get_descendant_regions(tocantins_province_id, REGION_LEVELS['district'])]\n",
    "print(districts_list_brazil)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Tocantins province NDVI and analyze for significance (this may take a minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Content\n",
      "No Content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tocantins Province in Brazil NDVI Anomalies \n",
      "Wanderlândia vegetation health (NDVI): 11% above average which is a significant high in 20 years \n",
      "Porto Nacional vegetation health (NDVI): 7% above average which is a significant high in 20 years \n",
      "Lajedão vegetation health (NDVI): 9% above average which is a significant high in 20 years \n",
      "Almas vegetation health (NDVI): 7% above average which is a significant high in 20 years \n",
      "Angico vegetation health (NDVI): 10% above average which is a significant high in 20 years \n",
      "Novo Acordo vegetation health (NDVI): 9% above average which is a significant high in 20 years \n",
      "Xambioá vegetation health (NDVI): 9% above average which is a significant high in 19 years \n",
      "Caseara vegetation health (NDVI): 7% above average which is a significant high in 20 years \n",
      "Riachinho vegetation health (NDVI): 9% above average which is a significant high in 20 years \n",
      "Dianopolis vegetation health (NDVI): 12% above average which is a significant high in 20 years \n",
      "Nazaré vegetation health (NDVI): 9% above average which is a significant high in 20 years \n",
      "Santa Terezinha do Tocantins vegetation health (NDVI): 11% above average which is a significant high in 20 years \n",
      "São Bento do Tocantins vegetation health (NDVI): 9% above average which is a significant high in 20 years \n",
      "Ananás vegetation health (NDVI): 8% above average which is a significant high in 20 years \n",
      "Itaguatins vegetation health (NDVI): 10% above average which is a significant high in 20 years \n",
      "Oliveira de Fátima vegetation health (NDVI): 7% above average which is a significant high in 19 years \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metric_id = 431132# Vegetation difference from 10-yr mean (2001-2010)\n",
    "item_id = 321 # Vegetation (NDVI)\n",
    "\n",
    "ndvi_array = get_data_for_multiple_regions(districts_list_brazil, metric_id, item_id)\n",
    "brazil_string = compute_amomalies(ndvi_array, \"Tocantins Province in Brazil NDVI Anomalies\")\n",
    "\n",
    "print(brazil_string)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
